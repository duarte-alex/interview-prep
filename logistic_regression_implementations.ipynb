{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c8f1051",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "Implement a gradient descent-based training algorithm for logistic regression. Your task is to compute model parameters using **Binary Cross Entropy (BCE) loss** and return the optimized coefficients along with collected loss values over iterations (rounded to the 4th decimal).\n",
    "\n",
    "---\n",
    "\n",
    "**Example**\n",
    "\n",
    "Input:\n",
    "```python\n",
    "train_logreg(np.array([[1.0, 0.5], [-0.5, -1.5], [2.0, 1.5], [-2.0, -1.0]]),\n",
    "             np.array([1, 0, 1, 0]),0.01,20)\n",
    "```\n",
    "\n",
    "Output:\n",
    "```python\n",
    "([0.0037, 0.0246, 0.0202],\n",
    " [2.7726, 2.7373, 2.7024, 2.6678, 2.6335, 2.5995, 2.5659,\n",
    "  2.5327, 2.4997, 2.4671, 2.4348, 2.4029, 2.3712, 2.3399,\n",
    "  2.3089, 2.2783, 2.2480, 2.2180, 2.1882, 2.1588])\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bd9c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n",
    "    \"\"\"\n",
    "    Gradient-descent training algorithm for logistic regression, optimizing parameters with Binary Cross Entropy loss.\n",
    "    \"\"\"\n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    y = y\n",
    "    X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "    B = np.zeros(X.shape[1])\n",
    "    losses = []\n",
    "\n",
    "    for _ in range(iterations):\n",
    "        y_pred = sigmoid(X @ B)\n",
    "        B -= learning_rate * X.T @ (y_pred - y)\n",
    "        loss = -np.sum(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n",
    "        losses.append(round(loss, 4))\n",
    "\n",
    "    return B.flatten().round(4).tolist(), losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b12691",
   "metadata": {},
   "source": [
    "### Logistic regression sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61db52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import numpy as np\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Explore C over a wide range, log-spaced\n",
    "C_values = np.logspace(-3, 2, 20)   # 0.001 â†’ 100, 20 values\n",
    "\n",
    "log_reg_params = {\n",
    "    'C': C_values,\n",
    "    'solver': ['lbfgs', 'liblinear'],\n",
    "    'penalty': ['l2', 'l1', 'none'],              \n",
    "    'class_weight': [None, 'balanced'],\n",
    "}\n",
    "\n",
    "log_reg_model = LogisticRegression(max_iter=3000, random_state=42)\n",
    "log_reg_search = RandomizedSearchCV(\n",
    "    estimator=log_reg_model,\n",
    "    param_distributions=log_reg_params,\n",
    "    n_iter=50,\n",
    "    scoring='roc_auc',\n",
    "    cv=5,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "log_reg_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Parameters for Logistic Regression:\", log_reg_search.best_params_)\n",
    "print(\"Best ROC-AUC Score for Logistic Regression:\", log_reg_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2faf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# after training input best parameters\n",
    "log_best = LogisticRegression(max_iter=3000, random_state=42, solver= 'lbfgs', C = 0.3)\n",
    "\n",
    "log_best.fit(X_train, y_train)\n",
    "\n",
    "y_test_probs_log = log_best.predict_proba(X_test)[:, 1]\n",
    "roc_auc_log = roc_auc_score(y_test, y_test_probs_log)\n",
    "fpr_log, tpr_log, _ = roc_curve(y_test, y_test_probs_log)\n",
    "\n",
    "fpr_log, tpr_log, _ = roc_curve(y_test, y_test_probs_log)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr_log, tpr_log, label=f'Logistic Regression Baseline (AUC = {roc_auc_log:.4f})')\n",
    "plt.xlabel('False Positive Rate (FPR)')\n",
    "plt.ylabel('True Positive Rate (TPR)')\n",
    "plt.title('ROC Curve for tunned on test data')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0f84ed",
   "metadata": {},
   "source": [
    "### class implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc67d096",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionScratch:\n",
    "    def __init__(self, learning_rate=0.01, iterations=1000):\n",
    "        self.lr = learning_rate\n",
    "        self.iterations = iterations\n",
    "        self.weights = None\n",
    "        self.bias = 0\n",
    "        self.cost_history = []\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        \"\"\"Sigmoid activation function\"\"\"\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def cost(self, h, y):\n",
    "        \"\"\"Cross-entropy loss\"\"\"\n",
    "        m = len(y)\n",
    "        return - (1/m) * np.sum(y*np.log(h) + (1-y)*np.log(1-h))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Train model using gradient descent\"\"\"\n",
    "        m, n = X.shape\n",
    "        self.weights = np.zeros(n)\n",
    "\n",
    "        for _ in range(self.iterations):\n",
    "            z = np.dot(X, self.weights) + self.bias\n",
    "            h = self.sigmoid(z)\n",
    "\n",
    "            dw = (1/m) * np.dot(X.T, (h - y))\n",
    "            db = (1/m) * np.sum(h - y)\n",
    "\n",
    "            self.weights -= self.lr * dw\n",
    "            self.bias -= self.lr * db\n",
    "\n",
    "            self.cost_history.append(self.cost(h, y))\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions\"\"\"\n",
    "        return (self.sigmoid(np.dot(X, self.weights) + self.bias) >= 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7ce675",
   "metadata": {},
   "source": [
    "### class implementation kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cc1775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/fareselmenshawii/logistic-regression-from-scratch?scriptVersionId=144686736&cellId=35\n",
    "class LogisticRegression:\n",
    "    \"\"\"\n",
    "    Logistic Regression model.\n",
    "\n",
    "    Parameters:\n",
    "        learning_rate (float): Learning rate for the model.\n",
    "\n",
    "    Methods:\n",
    "        initialize_parameter(): Initializes the parameters of the model.\n",
    "        sigmoid(z): Computes the sigmoid activation function for given input z.\n",
    "        forward(X): Computes forward propagation for given input X.\n",
    "        compute_cost(predictions): Computes the cost function for given predictions.\n",
    "        compute_gradient(predictions): Computes the gradients for the model using given predictions.\n",
    "        fit(X, y, iterations, plot_cost): Trains the model on given input X and labels y for specified iterations.\n",
    "        predict(X): Predicts the labels for given input X.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, learning_rate=0.0001):\n",
    "        np.random.seed(1)\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def initialize_parameter(self):\n",
    "        \"\"\"\n",
    "        Initializes the parameters of the model.\n",
    "        \"\"\"\n",
    "        self.W = np.zeros(self.X.shape[1])\n",
    "        self.b = 0.0\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Computes forward propagation for given input X.\n",
    "\n",
    "        Parameters:\n",
    "            X (numpy.ndarray): Input array.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Output array.\n",
    "        \"\"\"\n",
    "#         print(X.shape, self.W.shape)\n",
    "        Z = np.matmul(X, self.W) + self.b\n",
    "        A = sigmoid(Z)\n",
    "        return A\n",
    "\n",
    "    def compute_cost(self, predictions):\n",
    "        \"\"\"\n",
    "        Computes the cost function for given predictions.\n",
    "\n",
    "        Parameters:\n",
    "            predictions (numpy.ndarray): Predictions of the model.\n",
    "\n",
    "        Returns:\n",
    "            float: Cost of the model.\n",
    "        \"\"\"\n",
    "        m = self.X.shape[0]  # number of training examples\n",
    "        # compute the cost\n",
    "        cost = np.sum((-np.log(predictions + 1e-8) * self.y) + (-np.log(1 - predictions + 1e-8)) * (\n",
    "                1 - self.y))  # we are adding small value epsilon to avoid log of 0\n",
    "        cost = cost / m\n",
    "        return cost\n",
    "\n",
    "    def compute_gradient(self, predictions):\n",
    "        \"\"\"\n",
    "        Computes the gradients for the model using given predictions.\n",
    "\n",
    "        Parameters:\n",
    "            predictions (numpy.ndarray): Predictions of the model.\n",
    "        \"\"\"\n",
    "        # get training shape\n",
    "        m = self.X.shape[0]\n",
    "\n",
    "        # compute gradients\n",
    "        self.dW = np.matmul(self.X.T, (predictions - self.y))\n",
    "        self.dW = np.array([np.mean(grad) for grad in self.dW])\n",
    "\n",
    "        self.db = np.sum(np.subtract(predictions, self.y))\n",
    "\n",
    "        # scale gradients\n",
    "        self.dW = self.dW * 1 / m\n",
    "        self.db = self.db * 1 / m\n",
    "\n",
    "\n",
    "    def fit(self, X, y, iterations, plot_cost=True):\n",
    "        \"\"\"\n",
    "        Trains the model on given input X and labels y for specified iterations.\n",
    "\n",
    "        Parameters:\n",
    "            X (numpy.ndarray): Input features array of shape (n_samples, n )\n",
    "            y (numpy.ndarray): Labels array of shape (n_samples, 1)\n",
    "            iterations (int): Number of iterations for training.\n",
    "            plot_cost (bool): Whether to plot cost over iterations or not.\n",
    "\n",
    "        Returns:\n",
    "            None.\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "        self.initialize_parameter()\n",
    "\n",
    "        costs = []\n",
    "        for i in range(iterations):\n",
    "            # forward propagation\n",
    "            predictions = self.forward(self.X)\n",
    "\n",
    "            # compute cost\n",
    "            cost = self.compute_cost(predictions)\n",
    "            costs.append(cost)\n",
    "\n",
    "            # compute gradients\n",
    "            self.compute_gradient(predictions)\n",
    "\n",
    "            # update parameters\n",
    "            self.W = self.W - self.learning_rate * self.dW\n",
    "            self.b = self.b - self.learning_rate * self.db\n",
    "\n",
    "            # print cost every 100 iterations\n",
    "            if i % 10000 == 0:\n",
    "                print(\"Cost after iteration {}: {}\".format(i, cost))\n",
    "\n",
    "        if plot_cost:\n",
    "            fig = px.line(y=costs,title=\"Cost vs Iteration\",template=\"plotly_dark\")\n",
    "            fig.update_layout(\n",
    "                title_font_color=\"#41BEE9\", \n",
    "                xaxis=dict(color=\"#41BEE9\",title=\"Iterations\"), \n",
    "                yaxis=dict(color=\"#41BEE9\",title=\"cost\")\n",
    "            )\n",
    "            fig.show()\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts the labels for given input X.\n",
    "\n",
    "        Parameters:\n",
    "            X (numpy.ndarray): Input features array.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Predicted labels.\n",
    "        \"\"\"\n",
    "        predictions = self.forward(X)\n",
    "        return np.round(predictions)\n",
    "    \n",
    "    \n",
    "    def save_model(self, filename=None):\n",
    "        \"\"\"\n",
    "        Save the trained model to a file using pickle.\n",
    "\n",
    "        Parameters:\n",
    "            filename (str): The name of the file to save the model to.\n",
    "        \"\"\"\n",
    "        model_data = {\n",
    "            'learning_rate': self.learning_rate,\n",
    "            'W': self.W,\n",
    "            'b': self.b\n",
    "        }\n",
    "\n",
    "        with open(filename, 'wb') as file:\n",
    "            pickle.dump(model_data, file)\n",
    "\n",
    "    @classmethod\n",
    "    def load_model(cls, filename):\n",
    "        \"\"\"\n",
    "        Load a trained model from a file using pickle.\n",
    "\n",
    "        Parameters:\n",
    "            filename (str): The name of the file to load the model from.\n",
    "\n",
    "        Returns:\n",
    "            LogisticRegression: An instance of the LogisticRegression class with loaded parameters.\n",
    "        \"\"\"\n",
    "        with open(filename, 'rb') as file:\n",
    "            model_data = pickle.load(file)\n",
    "\n",
    "        # Create a new instance of the class and initialize it with the loaded parameters\n",
    "        loaded_model = cls(model_data['learning_rate'])\n",
    "        loaded_model.W = model_data['W']\n",
    "        loaded_model.b = model_data['b']\n",
    "\n",
    "        return loaded_model"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
