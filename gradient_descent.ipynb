{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f7e9fea",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "Write a Python function that performs linear regression using gradient descent. The function should take NumPy arrays `X` (features with a column of ones for the intercept) and `y` (target) as input, along with learning rate `alpha` and the number of iterations, and return the coefficients of the linear regression model as a NumPy array. Round your answer to four decimal places. `-0.0` is a valid result for rounding a very small number.\n",
    "\n",
    "---\n",
    "\n",
    "**Example:**\n",
    "\n",
    "**Input:**\n",
    "```python\n",
    "X = np.array([[1, 1], [1, 2], [1, 3]])\n",
    "y = np.array([1, 2, 3])\n",
    "alpha = 0.01\n",
    "iterations = 1000\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "```python\n",
    "np.array([0.1107, 0.9513])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a3077f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> np.ndarray:\n",
    "    m, n = X.shape\n",
    "    theta = np.zeros((n, 1))\n",
    "    for _ in range(iterations):\n",
    "        predictions = X @ theta\n",
    "        errors = predictions - y.reshape(-1, 1)\n",
    "        updates = X.T @ errors / m\n",
    "        theta -= alpha * updates\n",
    "    return np.round(theta.flatten(), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4e787c",
   "metadata": {},
   "source": [
    "### Using for loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e289c7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> np.ndarray:\n",
    "    m, n = X.shape\n",
    "    theta = np.zeros(n)\n",
    "\n",
    "    for it in range(iterations):\n",
    "        gradients = np.zeros(n)   # reset each iteration\n",
    "\n",
    "        for i in range(n):        # loop over features\n",
    "            gradient = 0\n",
    "            for j in range(m):    # loop over samples\n",
    "                prediction = X[j] @ theta      # dot product for sample j\n",
    "                error = prediction - y[j]\n",
    "                gradient += (1/m) * X[j][i] * error\n",
    "\n",
    "            gradients[i] = gradient\n",
    "\n",
    "        theta -= alpha * gradients\n",
    "\n",
    "    return np.round(theta.flatten(), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d480453c",
   "metadata": {},
   "source": [
    "### Gradient descent variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21005e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_gradient_descent(X, y, theta, lr=0.01, epochs=100):\n",
    "    m = len(y)\n",
    "    for _ in range(epochs):\n",
    "        gradients = (1/m) * X.T @ (X @ theta - y)\n",
    "        theta -= lr * gradients\n",
    "    return theta\n",
    "\n",
    "def stochastic_gradient_descent(X, y, theta, lr=0.01, epochs=100):\n",
    "    m = len(y)\n",
    "    for _ in range(epochs):\n",
    "        for i in range(m):\n",
    "            xi = X[i:i+1]\n",
    "            yi = y[i:i+1]\n",
    "            gradient = xi.T @ (xi @ theta - yi)\n",
    "            theta -= lr * gradient\n",
    "    return theta\n",
    "\n",
    "def mini_batch_gradient_descent(X, y, theta, lr=0.01, epochs=100, batch_size=32):\n",
    "    m = len(y)\n",
    "    for _ in range(epochs):\n",
    "        indices = np.random.permutation(m)\n",
    "        X_shuffled, y_shuffled = X[indices], y[indices]\n",
    "        for i in range(0, m, batch_size):\n",
    "            xb = X_shuffled[i:i+batch_size]\n",
    "            yb = y_shuffled[i:i+batch_size]\n",
    "            gradient = (1/len(yb)) * xb.T @ (xb @ theta - yb)\n",
    "            theta -= lr * gradient\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46330f2f",
   "metadata": {},
   "source": [
    "### without bias column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749348eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> np.ndarray:\n",
    "    m, n = X.shape\n",
    "    theta = np.zeros((n, 1))\n",
    "    b = 0.0\n",
    "    y = y.reshape(-1, 1)\n",
    "\n",
    "    for _ in range(iterations):\n",
    "        predictions = X @ theta + b\n",
    "        errors = predictions - y\n",
    "        grad_theta = (X.T @ errors) / m\n",
    "        grad_b = errors.mean()\n",
    "\n",
    "        theta -= alpha * grad_theta\n",
    "        b -= alpha * grad_b\n",
    "\n",
    "    return np.round(np.concatenate(([b], theta.flatten())), 4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3b3242",
   "metadata": {},
   "source": [
    "### Gradient descent formulas\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{2m} \\sum^{m}_{i = 1} (X \\theta_i - y_i)^2 = \\frac{1}{2m} \\sum^{m}_{i = 1} \\sum^{m}_{j = 1} (X_{ij} \\theta_j - y_i)^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial \\theta_k} = \\frac{1}{2m} \\sum^{m}_{i = 1} 2 ((X \\theta _i - y_i)) \\cdot \\frac{\\partial (X \\theta)_i}{\\partial \\theta_k} = \\frac{1}{m} \\sum^{m}_{i = 1} (X \\theta - y)_i X_{ik}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33c1ddd",
   "metadata": {},
   "source": [
    "### K means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677a983b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "def euclidean_distance(a, b):\n",
    "    return np.sqrt(((a - b) ** 2).sum(axis=1))\n",
    "\n",
    "def k_means_clustering(points, k, initial_centroids, max_iterations):\n",
    "    points = np.array(points)\n",
    "    centroids = np.array(initial_centroids)\n",
    "    \n",
    "    for iteration in range(max_iterations):\n",
    "        # Assign points to the nearest centroid\n",
    "        distances = np.array([euclidean_distance(points, centroid) for centroid in centroids])\n",
    "        assignments = np.argmin(distances, axis=0)\n",
    "\n",
    "        new_centroids = np.array([points[assignments == i].mean(axis=0) if len(points[assignments == i]) > 0 else centroids[i] for i in range(k)])\n",
    "        \n",
    "        # Check for convergence\n",
    "        if np.all(centroids == new_centroids):\n",
    "            break\n",
    "        centroids = new_centroids\n",
    "        centroids = np.round(centroids,4)\n",
    "    return [tuple(centroid) for centroid in centroids]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf247abb",
   "metadata": {},
   "source": [
    "### theta0 and theta1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1785b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, alpha=0.01, iters=1000):\n",
    "    m = len(y)\n",
    "    theta0, theta1 = 0, 0\n",
    "\n",
    "    for _ in range(iters):\n",
    "        prediction = theta0 + theta1 * X\n",
    "        error = prediction - y\n",
    "        \n",
    "        # Compute gradients\n",
    "        d_theta0 = (1/m) * sum(error)\n",
    "        d_theta1 = (1/m) * sum(error * X)\n",
    "        \n",
    "        # Update parameters\n",
    "        theta0 -= alpha * d_theta0\n",
    "        theta1 -= alpha * d_theta1\n",
    "    \n",
    "    return theta0, theta1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mpm2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
